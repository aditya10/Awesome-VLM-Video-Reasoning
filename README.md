# Awesome-VLM-Video-Reasoning  [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Most Video-LLMs can understand video scenes and caption them with some accuracy. However, can your video model go beyond that, and perform _complex reasoning tasks on surprising events_, _understand spatial relationships_, and _comprehend humor_? This repository contains benchmarks that challenge Multi-frame and Video-LLMs in terms of accurate perception üëÅÔ∏è, comprehension üí°, and reasoning üßê on the scene.

**Our paper:**

### üî• [Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events](https://blackswan.cs.ubc.ca)


*[Aditya Chinchure](https://www.adityachinchure.com/)<sup>1,2\*</sup>,  [Sahithya Ravi](https://sahithyaravi.github.io/)<sup>1,2\*</sup>,  [Raymond Ng](https://www.cs.ubc.ca/people/raymond-ng)<sup>1</sup>,  [Vered Shwartz](https://www.cs.ubc.ca/~vshwartz/)<sup>1,2</sup>,  [Boyang Li](http://www.boyangli.org/index.html)<sup>3</sup>,  [Leonid Sigal](https://www.cs.ubc.ca/~lsigal/index.html)<sup>1,2</sup>* (<sup>\*</sup>Indicates Equal Contribution)

*<sup>1</sup>University of British Columbia, <sup>2</sup>Vector Institute for AI, <sup>3</sup>Nanyang Technological University*

<h5>  
  
 **[Paper](https://openaccess.thecvf.com/content/CVPR2025/html/Chinchure_Black_Swan_Abductive_and_Defeasible_Video_Reasoning_in_Unpredictable_Events_CVPR_2025_paper.html)** | **[arXiv](https://arxiv.org/abs/2412.05725)** | **[Project Page](https://blackswan.cs.ubc.ca)**

</h5>

## Table of Contents

- [Benchmarking Video Models](#benchmarking-video-models)
- [Contributing](#contributing)

## Benchmarking Video Models

| Paper | Venue (Year) | Link |
| :----- | :------------: | :----: |
| [**Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events**](https://arxiv.org/abs/2412.05725) | CVPR 2025 | [Website](https://blackswan.cs.ubc.ca), [Code](https://github.com/sahithyaravi/BlackSwan) |
| [**FunQA: Towards Surprising Video Comprehension**](https://funqa-benchmark.github.io) | ECCV 2024 | [Website](https://funqa-benchmark.github.io) |
| [**Oops! Predicting Unintentional Action in Video**](https://oops.cs.columbia.edu) | CVPR 2020 | [Website](https://oops.cs.columbia.edu)) |
| [**Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences**](https://arxiv.org/abs/2001.06891) | CVPR 2020 | [Code](https://github.com/Guaranteer/VidSTG-Dataset) |
| [**UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces**](https://embodiedcity.github.io/UrbanVideo-Bench/) | arXiv 2025 | [Website](https://embodiedcity.github.io/UrbanVideo-Bench/) |
| [**OpenEQA: Embodied Question Answering in the Era of Foundation Models**](https://open-eqa.github.io) | CVPR 2024 | [Website](https://open-eqa.github.io) |
 [**MVBench: A Comprehensive Multi-modal Video Understanding Benchmark**](https://arxiv.org/abs/2311.17005) | arXiv 2023 | [Code](https://github.com/OpenGVLab/Ask-Anything) |
 | [**TVQA: Localized, Compositional Video Question Answering**](https://arxiv.org/abs/1809.01696) | EMNLP 2018 | [Website](https://tvqa.cs.unc.edu/) |
| [**Perception Test: A Diagnostic Benchmark for Multimodal Video Models**](https://arxiv.org/abs/2305.13786) | NeurIPS 2023 / ICCV 2023 Workshop | [Code](https://github.com/google-deepmind/perception_test) |
| [**TempCompass: Do Video LLMs Really Understand Videos?**](https://arxiv.org/abs/2403.00476) | ACL 2024 | [Code](https://github.com/llyx97/TempCompass) |
| [**Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis**](https://arxiv.org/abs/2405.21075) | arXiv 2024 | [Code](https://github.com/BradyFU/Video-MME) |
| [**VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models**](https://arxiv.org/abs/2406.16338) | arXiv 2024 | [Code](https://github.com/patrick-tssn/VideoHallucer) |
| [**TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering**](https://arxiv.org/abs/1704.04497) | CVPR 2017 | [Code](https://github.com/YunseokJANG/tgif-qa) |



## Contributing

We greatly appreciate your contributions! Please create a PR with your paper added to the list, in the same format as above. 

